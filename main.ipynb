{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4bc75b-c60e-4994-93ce-c9ff81406390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 15:55:11 WARN Utils: Your hostname, gimsehyeon-ui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.99 instead (on interface en0)\n",
      "24/10/03 15:55:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/03 15:55:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"chapter7\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ef4fd3-177a-4500-971d-0bd2d92f8e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferschema\", \"true\") \\\n",
    ".load(\"./data/retail-data/all/*.csv\") \\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c630e0cd-863e-46df-ada5-a2c75f3a4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4514e-6dd1-404f-a009-bda15523aa9c",
   "metadata": {},
   "source": [
    "## 집계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83693aa7-d662-4147-9ab5-b1ef13aa0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 특정 column을 count\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ffa914-b34f-48c7-a740-04faffa52a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e29c6f4-f3c3-4e9a-bb6d-288560bd0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec9e582-3636-42a8-b422-0b5bffe31f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# avg\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")) \\\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdfe61b-7918-4c41-ae9e-4ad0fe9059cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660879| 47559.39140929848|  218.08095663447733|   218.08115785023355|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분산과 표준 편차\n",
    "# 분산: 평균과의 차이를 제곱한 결과의 평균\n",
    "# 표준편차: 분산의 제곱근\n",
    "\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6e2d62-5163-4c62-a88c-1a4add35cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복합 데이터 타입의 집계\n",
    "# 스파크는 수식을 이용한 집계 뿐만 아니라 복합 데이터 타입을 사용해 집계를 수행할 수 있다. 예를 들어 특정 컬럼의 값을 리스트로 수집하거나 셋 데이터 타입으로 고윳값을 수집할 수 있다. \n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279904e4-f11e-41d9-b093-93c449c8d866",
   "metadata": {},
   "source": [
    "## 그룹화\n",
    "\n",
    "지금까지 DataFrame 수준의 집계만 다뤘다. 하지만 데이터 **그룹** 기반의 집계를 수행하는 경우가 더 많다. 데이터 그룹 기반의 집계는 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해서 계산하기 위해 카테고리형 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d0a8fcc-71de-4cd6-9a35-58d79f456ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 표현식을 이용한 그룹화\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e5c0191-620e-4439-ac87-b49961503e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 맵을 이용한 그룹화\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"), expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae202619-06eb-4a27-9b4a-1197ffcd727a",
   "metadata": {},
   "source": [
    "## 윈도우 함수\n",
    "\n",
    "**윈도우 함수**를 집계에 사용할 수도 있다. 윈도우 함수는 데이터의 특정 윈도우를 대상으로 고유의 집계 연산을 수행한다. 데이터의 윈도우는 현재 데이터에 대한 참조를 사용해 정의한다.\n",
    "\n",
    "윈도우 명세는 함수에 전달될 로우를 결정한다. 표준 group-by 함수와 유사해 보일수도 있으므로 차이점을 알아본다.\n",
    "\n",
    "group-by 함수를 사용하면 모든 로우 레코드가 단일 그룹으로만 이동한다. 윈도우 함수는 프레임에 입력되는 모든 로우에 대해 결과값을 계산한다. \n",
    "\n",
    "프레임은 로우 그룹 기반의 테이블을 의미한다. 각 로우는 하나 이상의 프레임에 할당될 수 있다. 가장 흔하게 사용되는 방법 중 하나는 하루를 나타내는 값의 롤링 평균을 구하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "927c7dcb-97b5-4d87-be1e-03226d8de5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주문 일자(InvoiceDate) 컬럼을 변환해 date 컬럼을 만든다. 이 컬럼은 시간 정보를 제외한 날짜 정보만을 가진다.\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(to_timestamp(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876e04d-e01f-4268-9858-a637f2321ee7",
   "metadata": {},
   "source": [
    "윈도우 함수를 정의하기 위해 첫 번째로 윈도우 명세를 만든다. 여기서 사용하는 `partitionBy` 메서드는 지금까지 사용해온 파티셔닝 스키마의 개념과는 관련이 없으며 그룹을 어떻게 나눌지 결정하는 것과 유사한 개념이다.\n",
    "\n",
    "`orderBy` 메서드는 파티션의 정렬 방식을 정의한다. 그리고 프레임 명세(`rowsBetween` 구문)는 입력된 로우의 참조를 기반으로 프레임에 로우가 포함될 수 있는지 결정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be720a61-4d1c-49ea-bc23-60120dc2cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window \\\n",
    ".partitionBy(\"CustomerId\", \"date\") \\\n",
    ".orderBy(desc(\"Quantity\")) \\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716531ef-bd49-4b3d-8b42-df4cd6effa57",
   "metadata": {},
   "source": [
    "이제 집계 함수를 사용해 고객을 좀 더 자세히 살펴본다. 여기서는 시간대별 최대 구매 개수를 구하는 예제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f1ea0a4-d5fb-47a3-b1fb-5b9825977373",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e85a53d4-4329-4a3e-9ab6-b13e5e097df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24d5d2b9-3510-4f40-89e4-e576ee2c70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05b367-73ff-4725-b812-a52aaef458a7",
   "metadata": {},
   "source": [
    "## 그룹화 셋\n",
    "\n",
    "지금까지는 컬럼의 값을 이용해 여러 컬럼을 집계하는 데 `group-by` 표현식을 사용했다. 때로는 여러 그룹에 걸쳐 집계할 수 있는 무언가가 필요할 수 있다.\n",
    "\n",
    "**그룹화 셋**이 바로 그 주인공이다. 그룹화 셋은 여러 집계를 결합하는 저수준 기능이다. 그룹화 셋을 이용하면 `group-by` 구문에서 원하는 형태로 집계를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad094c-63d1-471b-8cac-30453da8514b",
   "metadata": {},
   "source": [
    "### 롤업\n",
    "\n",
    "다양한 컬럼을 그룹화 키로 설정하면 그룹화 키로 설정된 조합 뿐만 아니라 데이터셋에서 볼 수 있는 실제 조합을 모두 살펴볼 수 있다. \n",
    "\n",
    "롤업은 `group-by` 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능이다.\n",
    "\n",
    "다음 예제에서는 시간(신규 Date 컬럼)과 공간(Country 컬럼)을 축으로 하는 롤업을 생성한다. 롤업의 결과로 생성된 DataFrame은 모든 날짜의 총합, 날짜별 총합, 날짜별 국가별 총합을 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c3fd39a-6635-4e2b-a463-027fcd8db8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1661c5c-b07c-4a70-9856-11bd4468c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      NULL|          NULL|       4906888|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|          NULL|         24032|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|          NULL|         20855|\n",
      "|2010-12-02|United Kingdom|         20705|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|        Poland|           140|\n",
      "|2010-12-03|         Spain|           400|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|          NULL|         11548|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\")) \\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\") \\\n",
    ".orderBy(\"Date\")\n",
    "\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38cc46-9815-40ff-b4e2-6d30b78619c7",
   "metadata": {},
   "source": [
    "### 큐브\n",
    "\n",
    "큐브는 롤업을 고차원적으로 사용할 수 있게 해준다. 큐브는 요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업을 수행한다. 즉, 전체 기간에 대해 날짜와 국가별 결과를 얻을 수 있다.\n",
    "\n",
    "메서드 호출 방식은 롤업과 매우 유사하며 `rollup`메서드 대신 `cube` 메서드를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61552885-1f4b-494e-a7e9-4d18496a4ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|NULL|              France|       109848|\n",
      "|NULL|               Italy|         7999|\n",
      "|NULL|             Finland|        10666|\n",
      "|NULL|           Lithuania|          652|\n",
      "|NULL|              Poland|         3653|\n",
      "|NULL|             Iceland|         2458|\n",
      "|NULL|               Japan|        25218|\n",
      "|NULL|United Arab Emirates|          982|\n",
      "|NULL|      United Kingdom|      4008533|\n",
      "|NULL|              Greece|         1556|\n",
      "|NULL|      Czech Republic|          592|\n",
      "|NULL|  European Community|          497|\n",
      "|NULL|              Sweden|        35637|\n",
      "|NULL|             Lebanon|          386|\n",
      "|NULL|              Canada|         2763|\n",
      "|NULL|           Australia|        83653|\n",
      "|NULL|             Denmark|         8188|\n",
      "|NULL|        Saudi Arabia|           75|\n",
      "|NULL|                NULL|      4906888|\n",
      "|NULL|           Singapore|         5234|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))) \\\n",
    ".select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bff9ba-d600-434a-ba98-785869148c06",
   "metadata": {},
   "source": [
    "### 피벗\n",
    "\n",
    "피벗을 사용해 로우를 컬럼으로 변환할 수 있다. 현재 데이터셋에서는 Country 컬럼이 있다. 피벗을 사용해 국가별로 집계 함수를 적용할 수 있으며 쿼리를 사용해 쉽게 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a4a8627-4c1d-4bf8-a12a-e825895c2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7386845-21a8-4991-a7c1-ebcf059c4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             NULL|\n",
      "|2011-12-09|             NULL|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             NULL|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50a8a2-c913-4440-9276-2d2103740bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
